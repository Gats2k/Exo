import logging
import time
from typing_extensions import override
from openai import AssistantEventHandler
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)

def prepare_messages_for_api(
    messages: List[Dict[str, str]], 
    current_model: str,
    system_instructions: Optional[str] = None
) -> List[Dict[str, str]]:
    """
    Pr√©pare l'historique des messages pour l'API IA en g√©rant les sp√©cificit√©s de chaque mod√®le.

    Args:
        messages: Liste de dictionnaires {"role": "user/assistant", "content": "..."}
        current_model: Le mod√®le actuellement utilis√© (ex: 'deepseek-reasoner', 'openai', etc.)
        system_instructions: Instructions syst√®me √† ajouter au d√©but (optionnel)

    Returns:
        Liste de messages format√©e et corrig√©e pour l'API
    """
    # Ajouter les instructions syst√®me au d√©but si fournies
    formatted_messages = []
    if system_instructions:
        formatted_messages.append({"role": "system", "content": system_instructions})

    # Si pas de messages ou mod√®le non concern√©, retourner tel quel
    if not messages or current_model != 'deepseek-reasoner':
        formatted_messages.extend(messages)
        return formatted_messages

    # CORRECTION 1 : V√©rifier et corriger les r√¥les initiaux pour deepseek-reasoner
    start_index = 0
    first_message_role = messages[0]['role'] if messages else None

    # Si le premier message est 'assistant', trouver le premier 'user' et supprimer ce qui pr√©c√®de
    if first_message_role == 'assistant':
        logger.warning("Premier message est 'assistant', correction n√©cessaire pour deepseek-reasoner")
        first_user_index = -1
        for i, msg in enumerate(messages):
            if msg['role'] == 'user':
                first_user_index = i
                break

        if first_user_index != -1:
            logger.info(f"Suppression de {first_user_index} message(s) 'assistant' initiaux")
            messages = messages[first_user_index:]
        else:
            # Aucun message user trouv√©, ajouter un message fictif
            logger.warning("Aucun message 'user' trouv√©, ajout d'un message fictif")
            messages.insert(0, {"role": "user", "content": "Bonjour"})

    # CORRECTION 2 : Fusionner les messages cons√©cutifs du m√™me r√¥le
    if len(messages) > 1:
        logger.info("Fusion des messages cons√©cutifs pour deepseek-reasoner")
        merged_messages = []

        if messages:
            merged_messages.append(messages[0])

            for i in range(1, len(messages)):
                current_message = messages[i]
                last_merged_message = merged_messages[-1]

                # Fusionner si m√™me r√¥le et pas 'system'
                if current_message['role'] == last_merged_message['role'] and current_message['role'] != 'system':
                    merged_content = f"{last_merged_message['content']}\n\n{current_message['content']}"
                    merged_messages[-1]['content'] = merged_content
                    logger.debug(f"Fusionn√© message {i} (role: {current_message['role']})")
                else:
                    merged_messages.append(current_message)

        formatted_messages.extend(merged_messages)
        logger.info(f"Historique final: {len(formatted_messages)} messages apr√®s fusion")
    else:
        formatted_messages.extend(messages)

    return formatted_messages

def execute_chat_completion(
    messages_history: List[Dict[str, str]],
    current_model: str,
    stream: bool = False,
    socketio_emitter = None,
    message_id = None,
    add_system_instructions: bool = True,  # <-- NOUVEAU PARAM√àTRE
    context: str = 'chat'  # <-- NOUVEAU: contexte pour les instructions
) -> Optional[str]:
    """
    Ex√©cute un appel Chat Completion pour les mod√®les non-OpenAI.

    Args:
        messages_history: Historique [{"role": "...", "content": "..."}]
        current_model: Le mod√®le actuel (deepseek, qwen, gemini, etc.)
        stream: Mode streaming (True pour web, False pour bots)
        socketio_emitter: Objet socketio pour √©mettre (si stream=True)
        message_id: ID du message pour l'√©mission (si stream=True)
        add_system_instructions: Si True, ajoute les instructions syst√®me par d√©faut.
        context: Contexte d'utilisation ('chat' ou 'lesson')

    Returns:
        - Si stream=False: retourne la r√©ponse compl√®te (string)
        - Si stream=True: retourne la r√©ponse compl√®te apr√®s streaming (string)
    """
    try:
        # 1. R√©cup√©rer client et mod√®le
        from ai_config import get_ai_client, get_model_name, get_system_instructions

        ai_client = get_ai_client()
        model_name = get_model_name()

        if not model_name:
            logger.error(f"Could not determine model name for {current_model}")
            raise ValueError(f"Model name not found for {current_model}")

        # 2. Pr√©parer les messages en ajoutant les instructions syst√®me SEULEMENT SI DEMAND√â
        system_prompt = get_system_instructions(context=context) if add_system_instructions else None

        final_messages = prepare_messages_for_api(
            messages_history,
            current_model,
            system_prompt
        )

        # 3. Appeler l'API
        logger.debug(f"Calling API with model={model_name}, stream={stream}, context={context}")
        response = ai_client.chat.completions.create(
            model=model_name,
            messages=final_messages,
            stream=stream
        )

        # 4. G√©rer la r√©ponse selon le mode (logique inchang√©e)
        if not stream:
            # Mode non-streaming (WhatsApp, Telegram)
            assistant_message = response.choices[0].message.content
            logger.info(f"Non-streaming response received from {current_model}")
            return assistant_message
        else:
            # Mode streaming (Web App)
            assistant_message = ""

            for chunk in response:
                chunk_content = None
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta and hasattr(delta, 'content'):
                        chunk_content = delta.content

                if chunk_content:
                    # Nettoyer le chunk
                    cleaned_chunk = _clean_response_text(chunk_content)
                    assistant_message += cleaned_chunk

                    # √âmettre via SocketIO si disponible
                    if socketio_emitter and message_id:
                        socketio_emitter.emit('response_stream', {
                            'content': cleaned_chunk,
                            'message_id': message_id,
                            'is_final': False
                        })

            # √âmettre le signal final
            if socketio_emitter and message_id:
                socketio_emitter.emit('response_stream', {
                    'content': '',
                    'message_id': message_id,
                    'is_final': True,
                    'full_response': assistant_message
                })

            logger.info(f"Streaming response completed from {current_model}")
            return assistant_message

    except Exception as e:
        logger.error(f"Error in execute_chat_completion: {str(e)}", exc_info=True)
        raise


def _clean_response_text(text: str) -> str:
    """
    Nettoie le texte en supprimant les caract√®res sp√©ciaux de formatage.

    Args:
        text: Le texte √† nettoyer

    Returns:
        Le texte nettoy√©
    """
    if not text:
        return text

    # Supprimer les ast√©risques, di√®ses, et autres caract√®res de formatage
    cleaned_text = text.replace('*', '').replace('#', '').replace('```', '').replace('---', '')
    return cleaned_text

def upload_image_to_openai(file_path: str, platform: str = "General") -> str:
    """
    Upload une image vers OpenAI et retourne l'ID du fichier

    Args:
        file_path: Chemin vers le fichier image local
        platform: Contexte d'origine ('Web', 'WhatsApp', 'Telegram', etc.)

    Returns:
        str: L'ID du fichier upload√© sur OpenAI

    Raises:
        Exception: Si l'upload √©choue
    """
    from ai_config import openai_client

    try:
        with open(file_path, 'rb') as file_content:
            openai_file = openai_client.files.create(
                file=file_content,
                purpose='assistants'
            )
            logger.info(f"Image {platform} upload√©e vers OpenAI avec ID: {openai_file.id}")
            return openai_file.id
    except Exception as e:
        logger.error(f"Erreur upload OpenAI ({platform}): {str(e)}")
        raise


def process_image_for_openai(
    file_path: str, 
    base64_data: str, 
    user_text: str = "",
    platform: str = "General"
) -> tuple:
    """
    Traite une image pour OpenAI avec double approche (Vision API + OCR Mathpix)

    Args:
        file_path: Chemin vers le fichier image local
        base64_data: Donn√©es image en base64 pour Mathpix
        user_text: Message utilisateur ou caption √† combiner avec l'OCR
        platform: Contexte d'origine pour la journalisation

    Returns:
        tuple: (openai_file_id, enhanced_message, results_dict)
            - openai_file_id: ID du fichier sur OpenAI (ou None)
            - enhanced_message: Message enrichi avec OCR
            - results_dict: Dictionnaire avec les statuts de succ√®s

    Raises:
        Exception: Si les deux m√©thodes (Mathpix et OpenAI) √©chouent
    """
    results = {
        'mathpix_success': False,
        'openai_success': False,
        'formatted_summary': "",
        'openai_file_id': None
    }

    # 1. Mathpix OCR (silencieux si √©chec)
    try:
        from mathpix_utils import process_image_with_mathpix
        mathpix_result = process_image_with_mathpix(base64_data)
        if "error" not in mathpix_result:
            results['formatted_summary'] = mathpix_result.get("formatted_summary", "")
            results['mathpix_success'] = True
            logger.info(f"Mathpix OCR r√©ussi pour {platform}")
    except Exception as e:
        logger.error(f"√âchec Mathpix pour {platform}: {str(e)}")

    # 2. OpenAI Upload (silencieux si √©chec)
    try:
        results['openai_file_id'] = upload_image_to_openai(file_path, platform)
        results['openai_success'] = True
        logger.info(f"Upload OpenAI r√©ussi pour {platform}: {results['openai_file_id']}")
    except Exception as e:
        logger.error(f"√âchec upload OpenAI {platform}: {str(e)}")

    # 3. Validation - Au moins une m√©thode doit r√©ussir
    if not results['mathpix_success'] and not results['openai_success']:
        raise Exception(f"Impossible de traiter l'image {platform}. Veuillez r√©essayer.")

    # 4. Construction du message enrichi
    enhanced_message = user_text or "Veuillez analyser cette image."
    if results['formatted_summary']:
        enhanced_message += f"\n\n[Extracted Image Content]\n{results['formatted_summary']}"

    return results['openai_file_id'], enhanced_message, results

class OpenAIAssistantEventHandler(AssistantEventHandler):
    """Gestionnaire d'√©v√©nements pour le streaming des r√©ponses de l'Assistant OpenAI"""

    def __init__(self, socket, message_id):
        super().__init__()
        self.socket = socket
        self.message_id = message_id
        self.full_response = ""
        self._AssistantEventHandler__stream = None
        self.time_module = time
        self.run_id = None

    @override
    def on_event(self, event):
        if event.event == 'thread.run.created':
            self.run_id = event.data.id
            logger.info(f"EventHandler: Run cr√©√© avec ID: {self.run_id}")

    @override
    def on_text_created(self, text) -> None:
        # Initialisation du texte - pas besoin d'envoyer de contenu ici
        pass

    @override
    def on_text_delta(self, delta, snapshot):
        # Ajouter le delta au texte complet
        self.full_response += delta.value

        # √âmettre le nouveau contenu √† l'utilisateur
        self.socket.emit(
            'response_stream', {
                'content': delta.value,
                'message_id': self.message_id,
                'is_final': False
            })

    @override
    def on_run_completed(self):
        # √âmettre l'√©v√©nement final quand le run est termin√©
        self.socket.emit(
            'response_stream', {
                'content': '',
                'message_id': self.message_id,
                'is_final': True,
                'full_response': self.full_response
            })

    @override
    def on_tool_call_created(self, tool_call):
        # Pour g√©rer les appels d'outils comme code_interpreter si n√©cessaire
        pass

    @override
    def on_tool_call_delta(self, delta, snapshot):
        # G√©rer les mises √† jour des appels d'outils
        if delta.type == 'code_interpreter':
            if delta.code_interpreter and delta.code_interpreter.input:
                self.full_response += f"\n```python\n{delta.code_interpreter.input}\n```\n"
                self.socket.emit(
                    'response_stream', {
                        'content':
                        f"\n```python\n{delta.code_interpreter.input}\n```\n",
                        'message_id': self.message_id,
                        'is_final': False
                    })

            if delta.code_interpreter and delta.code_interpreter.outputs:
                for output in delta.code_interpreter.outputs:
                    if output.type == "logs":
                        self.full_response += f"\n```\n{output.logs}\n```\n"
                        self.socket.emit(
                            'response_stream', {
                                'content': f"\n```\n{output.logs}\n```\n",
                                'message_id': self.message_id,
                                'is_final': False
                            })

def generate_reminder_message(
    user_identifier: str,
    platform: str,
    thread_id: str = None,
    reminder_type: str = "night"
) -> str:
    """
    G√©n√®re un message de rappel via le mod√®le configur√© en utilisant le syst√®me existant

    Args:
        user_identifier: Num√©ro WhatsApp ou telegram_id
        platform: 'whatsapp' ou 'telegram'
        thread_id: Thread ID de l'utilisateur (pour OpenAI ou autres mod√®les)
        reminder_type: 'night', 'morning', ou 'evening'

    Returns:
        str: Message de rappel personnalis√©
    """
    from ai_config import CURRENT_MODEL, ASSISTANT_ID, openai_client, CONTEXT_MESSAGE_LIMIT
    from models import User, UserMemory, TelegramMessage, WhatsAppMessage
    import time

    # Messages de consigne selon le type de rappel
    reminder_prompts = {
        'night': "En te basant sur nos √©changes d'aujourd'hui, envoie-moi un message de bonne nuit tr√®s court (1-2 phrases max, style nouchi) pour me souhaiter bonne nuit et c√©l√©brer mon travail de la journ√©e.Si on a rien fait aujourd'hui, souhaite moi simplement¬†bonne¬†nuit.",
        'morning': "Envoie-moi un message de bon matin tr√®s court (1-2 phrases max, style nouchi) pour me souhaiter bon courage pour la journ√©e.",
        'evening': "En te basant sur nos √©changes r√©cents, envoie-moi un message de motivation tr√®s court (1-2 phrases max, style nouchi) pour m'encourager √† travailler ce soir."
    }

    user_message = reminder_prompts.get(reminder_type, reminder_prompts['night'])

    try:
        # === R√âCUP√âRATION DU CONTEXTE M√âMOIRE (comme dans le code existant) ===
        memory_context = ""
        user_phone_id = f"{platform}_{user_identifier}"

        # Import local pour √©viter circularit√©
        from app import app
        with app.app_context():
            user = User.query.filter_by(phone_number=user_phone_id).first()
            if user:
                memory = UserMemory.query.filter_by(user_id=user.id).first()
                if memory:
                    derniers_sujets_str = str(memory.derniers_sujets[-2:]) if memory.derniers_sujets else "[]"
                    memory_context = (
                        f"[Contexte sur l'√©l√®ve : "
                        f"Nom='{memory.nom or 'Inconnu'}', "
                        f"Niveau='{memory.niveau or 'Inconnu'}', "
                        f"Mati√®res difficiles={memory.matieres_difficiles or '[]'}, "
                        f"Derniers sujets abord√©s={derniers_sujets_str}. "
                        f"Adapte tes r√©ponses √† ce contexte sans jamais le mentionner explicitement.]\n"
                        f"---\n"
                    )

        from ai_config import get_system_instructions
        base_instructions = get_system_instructions()
        final_system_prompt = memory_context + base_instructions

        # === LOGIQUE SELON LE MOD√àLE ===
        if CURRENT_MODEL == 'openai':
            # Utiliser l'Assistant OpenAI avec le thread existant
            if not thread_id:
                logger.error("Thread ID manquant pour OpenAI Assistant")
                return f"Yo poto! Bonne nuit! üò¥"

            # Ajouter le contexte + consigne au thread
            message_with_context = final_system_prompt + "\n\n---\n\n" + user_message

            openai_client.beta.threads.messages.create(
                thread_id=thread_id,
                role="user",
                content=message_with_context
            )

            # Cr√©er et ex√©cuter la run
            run = openai_client.beta.threads.runs.create(
                thread_id=thread_id,
                assistant_id=ASSISTANT_ID
            )

            # Attendre la compl√©tion (timeout 60s pour rappel)
            timeout = 60
            start_time = time.time()

            while True:
                if time.time() - start_time > timeout:
                    logger.error("Timeout g√©n√©ration rappel OpenAI")
                    raise TimeoutError("OpenAI response timed out")

                run_status = openai_client.beta.threads.runs.retrieve(
                    thread_id=thread_id,
                    run_id=run.id
                )

                if run_status.status == 'completed':
                    break
                elif run_status.status in ['failed', 'cancelled', 'expired']:
                    raise Exception(f"OpenAI Run failed: {run_status.status}")

                time.sleep(1)

            # R√©cup√©rer la r√©ponse
            messages = openai_client.beta.threads.messages.list(
                thread_id=thread_id,
                order='desc',
                limit=1
            )

            if messages.data and messages.data[0].role == 'assistant':
                response = messages.data[0].content[0].text.value
                logger.info(f"Message rappel g√©n√©r√© via OpenAI Assistant pour {platform}/{user_identifier}")
                return response
            else:
                raise Exception("Pas de r√©ponse assistant valide")

        else:
            # Utiliser Chat Completion pour les autres mod√®les
            # R√©cup√©rer l'historique r√©cent pour le contexte
            messages_history = []

            with app.app_context():
                if platform == 'telegram':
                    # Chercher la conversation Telegram
                    from models import TelegramConversation
                    conversation = TelegramConversation.query.filter_by(
                        thread_id=thread_id
                    ).first()

                    if conversation:
                        messages_query = TelegramMessage.query.filter_by(
                            conversation_id=conversation.id
                        ).order_by(TelegramMessage.created_at.desc()).limit(5).all()

                        for msg in reversed(messages_query):
                            role = msg.role if msg.role == 'user' else 'assistant'
                            messages_history.append({"role": role, "content": msg.content})

                elif platform == 'whatsapp':
                    messages_query = WhatsAppMessage.query.filter_by(
                        thread_id=thread_id
                    ).order_by(WhatsAppMessage.timestamp.desc()).limit(5).all()

                    for msg in reversed(messages_query):
                        role = 'user' if msg.direction == 'inbound' else 'assistant'
                        messages_history.append({"role": role, "content": msg.content})

            # Ajouter le message de rappel
            messages_history.append({"role": "user", "content": user_message})

            # Utiliser execute_chat_completion avec syst√®me existant
            response = execute_chat_completion(
                messages_history=messages_history,
                current_model=CURRENT_MODEL,
                stream=False,
                add_system_instructions=True  # Ajoute automatiquement memory + base_instructions
            )

            logger.info(f"Message rappel g√©n√©r√© via {CURRENT_MODEL} pour {platform}/{user_identifier}")
            return response

    except Exception as e:
        logger.error(f"Erreur g√©n√©ration message de rappel: {str(e)}", exc_info=True)
        # Fallback simple
        fallback_messages = {
            'night': "Yo poto! Bonne nuit! üò¥",
            'morning': "Yo poto! Bonne journ√©e! üí™",
            'evening': "Yo poto! C'est le moment de bosser! üî•"
        }
        return fallback_messages.get(reminder_type, fallback_messages['night'])